#!/bin/bash
# ========================================
# ðŸ“Š Data Lake Lab Setup Script (Enhanced)
# ========================================
# ðŸŽ¯ Purpose:
#   This script automates the setup of a **Data Lake Lab** AI environment, ensuring
#   all core tools, services, and dependencies are installed, configured, and ready 
#   for use while maintaining system hygiene and efficiency. Note, the AI components 
#   will be running on an Nvidia Orion Nano (primariluy Hugging Face for NLP and
#   vision experiments. The Datalake is based out of a external high performance
#   RAID drive surfaced inside a MinIo S3 storage layer.

# ðŸ› ï¸ Core System & Productivity Tools:
#   - **Deja Dup** â†’ Backup solution for safeguarding data.
#   - **Slack** â†’ Communication and team collaboration.
#   - **Git** â†’ Version control system for managing codebases.
#   - **Visual Studio Code** â†’ Lightweight IDE for development tasks.
#   - **Thunderbird** â†’ Email client for personal and professional use.

# ðŸ’¾ Data Lake Core Components:
#   - **MinIO** â†’ S3-compatible object storage for your data lake.
#   - **HashiCorp Vault** â†’ Secrets management and secure access control.
#   - **ClamAV** â†’ Antivirus scanner for data integrity and system security.

# ðŸ’¡ Development Ecosystem:
#   - **Node.js & NPM** â†’ JavaScript runtime for backend and frontend development.
#   - **Jekyll & Bundler** â†’ Static site generator for web development.
#   - **.NET SDK** â†’ .NET Core framework for cross-platform development.
#   - **Java 17** â†’ Required for Spark and other JVM-based tools.
#   - **PySpark** â†’ Python API for Apache Spark, enabling big data analytics.

# ðŸ“Š Data Engineering & Orchestration:
#   - **Apache Airflow** â†’ Workflow orchestration for ETL processes.
#   - **Trino** â†’ Distributed SQL query engine for fast data lake queries.

# ðŸ§¹ System Integrity & Maintenance:
#   - **Repository Cleanup** â†’ Detects and removes outdated or broken repositories.
#   - **Slack Installer Check** â†’ Skips reinstallation if Slack is already present.
#   - **Orphaned Package Removal** â†’ Keeps the system clean with autoremove.
#   - **Temporary File Cleanup** â†’ Removes leftover installation files post-setup.

# ðŸ“Œ Notes:
#   - The script checks for existing installations to avoid duplicates.
#   - It handles known repository issues and ensures system package integrity.
#   - Designed for **Pop!_OS 22.04** but adaptable to other Ubuntu-based systems.

# ðŸš€ Run with:
#   sudo ./datalake-lab-setup.sh

# ðŸŸ¢ Hardware Requirements (Jetson Orin Nano + Data Lake Workloads):
# - NVIDIA Jetson Orin Nano 4GB (Minimum) â€” 8GB or higher (Recommended)
# - External RAID Array (18TB WD Red Pro Drives) â€” for Parquet storage and raw data
# - 1TB NVMe SSD or MicroSD (for OS and local app data)
# - 5V 4A Power Supply (ensure stable power for Orin and peripherals)
# - At least 32GB swap space (to handle memory-intensive tasks with PySpark/Trino)

# ðŸŸ¢ Networking:
# - Gigabit Ethernet recommended (for MinIO and distributed queries)
# - Configure local network for internal IP addressing if clustering later

# --------------------------------------------------------------------------------------
# ðŸ“ MinIO â€” Object Storage for Parquet Files
# --------------------------------------------------------------------------------------
# - CPU: 2 cores minimum (4+ recommended for concurrent reads/writes)
# - RAM: 2GB minimum (4GB recommended for high I/O)
# - Disk: Direct access to RAID array for raw and processed data
# - Network: Gigabit Ethernet (to handle multiple ETL tasks)
# - Configuration:
#   - Run in distributed mode if scaling is planned
#   - Enable versioning and bucket policies for data governance
#   - S3-compatible â€” can integrate directly with PySpark and Trino

# --------------------------------------------------------------------------------------
# ðŸ”¥ PySpark â€” ETL & Batch Processing
# --------------------------------------------------------------------------------------
# - CPU: 4 cores minimum (8+ for large batch jobs)
# - RAM: 8GB minimum (16GB recommended for large datasets)
# - Disk: Access to RAID array and high-speed SSD for temp files
# - Java: OpenJDK 11 or 17 (compatible with Spark 3.x)
# - Configuration:
#   - Use Sparkâ€™s native support for Parquet files
#   - Set optimized partitioning strategies (avoid small file issues)
#   - Leverage GPU for ML/AI if extending Spark MLlib

# --------------------------------------------------------------------------------------
# â° Airflow â€” Workflow Orchestration
# --------------------------------------------------------------------------------------
# - CPU: 2 cores minimum (4+ recommended for heavy DAGs)
# - RAM: 4GB minimum (8GB recommended for concurrent tasks)
# - Database: PostgreSQL or MySQL backend for Airflow metadata (SQLite not recommended for production)
# - Web Server: Flask-based, runs on port 8080 by default
# - Scheduler: Handles task queues and DAG executions
# - Configuration:
#   - Use Celery Executor for distributed task management if scaling
#   - Set up environment variables for DAGs, connections, and secrets

# --------------------------------------------------------------------------------------
# ðŸ¦Œ Trino â€” Distributed SQL Query Engine
# --------------------------------------------------------------------------------------
# - CPU: 4 cores minimum (8+ for complex queries)
# - RAM: 8GB minimum (16GB recommended for complex joins and aggregations)
# - Disk: Fast SSD for query temp files and cache
# - Java: OpenJDK 17 (officially supported)
# - Configuration:
#   - Connect Trino to MinIO (as an S3-compatible storage) and query Parquet directly
#   - Set up catalogs for MinIO and local storage
#   - Enable parallel processing and optimize memory settings in `jvm.config`

# --------------------------------------------------------------------------------------
# âš¡ System-Wide Considerations:
# --------------------------------------------------------------------------------------
# - Docker (or Podman) for containerized deployments of MinIO, Airflow, and Trino
# - Use Nginx or Traefik as a reverse proxy if exposing services externally
# - GPU Acceleration: Use Orinâ€™s GPU for AI/ML tasks (if expanding PySpark or Trino ML capabilities)
# - Monitoring: Integrate Prometheus + Grafana for tracking resource usage and performance
# - Backup Strategies: Regular snapshots of Parquet datasets and Airflow metadata DB

# ðŸŸ¢ Recommended Deployment Order:
# 1. MinIO (to establish object storage)
# 2. PySpark (for ETL and data prep)
# 3. Airflow (to orchestrate workflows and automate tasks)
# 4. Trino (for federated querying across Parquet files and external data sources)


# ========================================
# Data Lake Lab Setup Script (Enhanced)
# Configures MinIO, HashiCorp Vault, and required dependencies
# ========================================

# Ensure the script runs as root
if [ "$(id -u)" -ne 0 ]; then
    echo "This script must be run as root. Try running: sudo $0"
    exit 1
fi

# -----------------------------------------------
# Remove OpenProject Repository Completely - oops
# -----------------------------------------------
echo "Searching for OpenProject repository references..."
openproject_files=$(grep -rl "openproject" /etc/apt/sources.list /etc/apt/sources.list.d/)

if [ -n "$openproject_files" ]; then
    echo "Removing OpenProject repository files..."
    for file in $openproject_files; do
        sudo rm "$file"
        echo "Removed $file"
    done
    sudo apt clean
    sudo apt update
else
    echo "No OpenProject repository references found."
fi

# ----------------------------------------
# Update System Packages
# ----------------------------------------
echo "Updating system packages..."
sudo apt update -y && sudo apt upgrade -y

# ----------------------------------------
# Install Deja Dup (Backup Tool)
# ----------------------------------------
echo "Installing Deja Dup..."
sudo apt install -y deja-dup

# ----------------------------------------
# Install Slack (with check)
# ----------------------------------------
if ! command -v slack &> /dev/null; then
    echo "Installing Slack..."
    wget -O /tmp/slack.deb https://downloads.slack-edge.com/releases/linux/4.36.140/prod/x64/slack-desktop-4.36.140-amd64.deb
    if [ -f /tmp/slack.deb ]; then
        sudo apt install -y /tmp/slack.deb
    else
        echo "Failed to download Slack. Skipping installation."
    fi
else
    echo "Slack is already installed. Skipping."
fi

# ----------------------------------------
# Install Git
# ----------------------------------------
if ! command -v git &> /dev/null; then
    echo "Installing Git..."
    sudo apt install -y git
else
    echo "Git is already installed."
fi

# ----------------------------------------
# Install Node.js and NPM
# ----------------------------------------
if ! command -v node &> /dev/null; then
    echo "Installing Node.js and NPM..."
    curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
    sudo apt install -y nodejs
else
    echo "Node.js is already installed."
fi

# ----------------------------------------
# Install Jekyll and Bundler
# ----------------------------------------
if ! gem list -i jekyll &> /dev/null; then
    echo "Installing Jekyll and Bundler..."
    sudo apt install -y ruby-full build-essential zlib1g-dev
    gem install jekyll bundler
else
    echo "Jekyll is already installed."
fi

# ----------------------------------------
# Install ClamAV (Antivirus Scanner)
# ----------------------------------------
if ! command -v clamscan &> /dev/null; then
    echo "Installing ClamAV..."
    sudo apt install -y clamav clamav-daemon
    sudo systemctl stop clamav-freshclam
    sudo freshclam
    sudo systemctl start clamav-freshclam
else
    echo "ClamAV is already installed."
fi

# ----------------------------------------
# Install MinIO
# ----------------------------------------
if ! command -v minio &> /dev/null; then
    echo "Installing MinIO..."
    wget https://dl.min.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio
    chmod +x /usr/local/bin/minio
else
    echo "MinIO is already installed."
fi

# ----------------------------------------
# Install HashiCorp Vault
# ----------------------------------------
if ! command -v vault &> /dev/null; then
    echo "Installing HashiCorp Vault..."
    wget https://releases.hashicorp.com/vault/1.14.4/vault_1.14.4_linux_amd64.zip -O /tmp/vault.zip
    unzip /tmp/vault.zip -d /usr/local/bin/
    chmod +x /usr/local/bin/vault
else
    echo "HashiCorp Vault is already installed."
fi

# ----------------------------------------
# Cleanup
# ----------------------------------------
echo "Cleaning up temporary files..."
sudo rm -f /tmp/slack.deb /tmp/vault.zip packages.microsoft.gpg packages-microsoft-prod.deb

echo "Data Lake Lab Setup Completed Successfully!"

